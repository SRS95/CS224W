{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/python2/lib/python2.7/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import snap\n",
    "import operator\n",
    "import snap\n",
    "from scipy.stats import binom\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "# Machine learning packages\n",
    "\n",
    "# Supervised learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Unsupervised learning (i.e. clustering)\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Clustering assessment metrics(for unknown ground truth)\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import calinski_harabaz_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNumAcquired(sim_rankings, acquired_companies_in_graph_by_id, k):\n",
    "    top_rankings = sim_rankings[:k]\n",
    "    result = 0\n",
    "    \n",
    "    for tup in top_rankings:\n",
    "        company_id = int(tup[0])\n",
    "        if company_id in acquired_companies_in_graph_by_id: result += 1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCompaniesToNumAcquired(k, node_to_sorted_L2_similarities, acquired_companies_in_graph_by_id, companies):\n",
    "    result = {}\n",
    "\n",
    "    for company_id in companies:\n",
    "        sim_rankings = node_to_sorted_L2_similarities[str(company_id)]\n",
    "        result[company_id] = getNumAcquired(sim_rankings, acquired_companies_in_graph_by_id, k)\n",
    "                                     \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNullProbs(companies_to_num_acquired, n, p):\n",
    "    result = {}\n",
    "    \n",
    "    for key in companies_to_num_acquired:\n",
    "        result[key] = binom.pmf(companies_to_num_acquired[key], n, p)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a list sorted from the node with the least l2 distance to \n",
    "# reference node to the node with the greatest l2 distance\n",
    "def calculateSortedL2Similarity(reference_node, model):\n",
    "    reference_embeddings = model[reference_node]\n",
    "    l2_similarities = {}\n",
    "\n",
    "    for key in model.vocab:\n",
    "        if key != reference_node:\n",
    "            curr_l2 = np.linalg.norm(reference_embeddings - model[key])\n",
    "            l2_similarities[key] = curr_l2\n",
    "\n",
    "    sorted_l2_similarities = sorted(l2_similarities.items(), key=operator.itemgetter(1))\n",
    "    return sorted_l2_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSortedSimilaritesMap(model):\n",
    "    result = {}\n",
    "    for key in model.vocab:\n",
    "        result[key] = calculateSortedL2Similarity(key, model)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embeddings from memory\n",
    "BFS_model = Word2Vec.load_word2vec_format(\"../node2vec_embeddings/company_embeddings_p1_q100_50iters.emd\")\n",
    "neutral_model = Word2Vec.load_word2vec_format(\"../node2vec_embeddings/company_embeddings_p1_q1_50iters.emd\")\n",
    "DFS_model = Word2Vec.load_word2vec_format(\"../node2vec_embeddings/company_embeddings_p1_q0_01_50iters.emd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "BFS_nodeToSortedL2Similarities = createSortedSimilaritesMap(BFS_model)\n",
    "print \"Completed BFS model\"\n",
    "neutral_nodeToSortedL2Similarities = createSortedSimilaritesMap(neutral_model)\n",
    "print \"Completed neutral model\"\n",
    "DFS_nodeToSortedL2Similarities = createSortedSimilaritesMap(DFS_model)\n",
    "print \"Completed DFS model\"\n",
    "'''\n",
    "BFS_nodeToSortedL2Similarities = np.load(\"../node2vec_embeddings/nodeToSortedL2Similarities_p1_q100.npy\")\n",
    "neutral_nodeToSortedL2Similarities = np.load(\"../node2vec_embeddings/nodeToSortedL2Similarities_p1_q1.npy\")\n",
    "DFS_nodeToSortedL2Similarities = np.load(\"../node2vec_embeddings/nodeToSortedL2Similarities_p1_q0_01.npy\")\n",
    "print BFS_nodeToSortedL2Similarities.keys()\n",
    "print \"here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT RUN THIS AGAIN!\n",
    "'''\n",
    "np.save(\"../node2vec_embeddings/nodeToSortedL2Similarities_p1_q100\", BFS_nodeToSortedL2Similarities)\n",
    "np.save(\"../node2vec_embeddings/nodeToSortedL2Similarities_p1_q1\", neutral_nodeToSortedL2Similarities)\n",
    "np.save(\"../node2vec_embeddings/nodeToSortedL2Similarities_p1_q0_01\", DFS_nodeToSortedL2Similarities)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all of the acquired companies we know about\n",
    "acquired_companies = set(np.load(\"../acquired_companies.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we find the overlap between companies in our graph and the set of acquired companies\n",
    "node_id_to_value = np.load(\"../../graphs/investors_to_companies_directed/node_id_to_value.npy\").item()\n",
    "FIn = snap.TFIn(\"../../graphs/investors_to_companies_directed/investors_to_companies_directed_folded_reverse_order.graph\")\n",
    "G = snap.TUNGraph.Load(FIn)\n",
    "\n",
    "companies_in_graph = set()\n",
    "for EI in G.Edges(): \n",
    "    curr_src_id = EI.GetSrcNId()\n",
    "    curr_dst_id = EI.GetDstNId()\n",
    "    companies_in_graph.add(node_id_to_value[curr_src_id])\n",
    "    companies_in_graph.add(node_id_to_value[curr_dst_id])\n",
    "    \n",
    "# These are the acquired companies in our folded graph\n",
    "acquired_companies_in_graph = companies_in_graph.intersection(acquired_companies)\n",
    "\n",
    "acquired_companies_in_graph_by_id = []\n",
    "value_to_node_id = {v: k for k, v in node_id_to_value.iteritems()}\n",
    "for company in acquired_companies_in_graph:\n",
    "    acquired_companies_in_graph_by_id.append(value_to_node_id[company])\n",
    "    \n",
    "    \n",
    "not_acquired_companies_in_graph = companies_in_graph.difference(acquired_companies)\n",
    "not_acquired_companies_in_graph_by_id = []\n",
    "for company in not_acquired_companies_in_graph:\n",
    "    not_acquired_companies_in_graph_by_id.append(value_to_node_id[company])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to look at companies that are NOT acquired too\n",
    "\n",
    "k = 500\n",
    "acquired_to_num_acquired_BFS = getCompaniesToNumAcquired(k, BFS_nodeToSortedL2Similarities, acquired_companies_in_graph_by_id, acquired_companies_in_graph_by_id)\n",
    "acquired_to_num_acquired_neutral = getCompaniesToNumAcquired(k, neutral_nodeToSortedL2Similarities, acquired_companies_in_graph_by_id, acquired_companies_in_graph_by_id)\n",
    "acquired_to_num_acquired_DFS = getCompaniesToNumAcquired(k, DFS_nodeToSortedL2Similarities, acquired_companies_in_graph_by_id, acquired_companies_in_graph_by_id)\n",
    "# Make a plot for each of BFS, neutral, and DFS\n",
    "# x axis will be node id\n",
    "# y axis will be probability under null (binomial) model\n",
    "p = float(len(acquired_companies_in_graph_by_id)) / len(companies_in_graph)\n",
    "BFS_null_model_probabilities = getNullProbs(acquired_to_num_acquired_BFS, k, p)\n",
    "neutral_null_model_probabilities = getNullProbs(acquired_to_num_acquired_neutral, k, p)\n",
    "DFS_null_model_probabilities = getNullProbs(acquired_to_num_acquired_DFS, k, p)\n",
    "\n",
    "\n",
    "print np.mean(acquired_to_num_acquired_BFS.values())\n",
    "print np.mean(acquired_to_num_acquired_neutral.values())\n",
    "print np.mean(acquired_to_num_acquired_DFS.values())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = range(len(acquired_companies_in_graph_by_id))\n",
    "x_vals = sorted(acquired_companies_in_graph_by_id)\n",
    "\n",
    "y_vals = []\n",
    "outliers = []\n",
    "for val in x_vals:\n",
    "    y_val = acquired_to_num_acquired_DFS[val]\n",
    "    y_vals.append(y_val)\n",
    "    if y_val >= 20: \n",
    "        outliers.append(val)\n",
    "\n",
    "plt.xlabel(\"Node ID\")\n",
    "plt.ylabel(\"Number acquired companies in top \" + str(k))\n",
    "plt.title(\"Nodes vs. number acquired in top \" + str(k) + \" most similar embeddings (DFS)\")\n",
    "plt.scatter(X, y_vals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to look at companies that are NOT acquired too\n",
    "\n",
    "k = 500\n",
    "not_acquired_to_num_acquired_BFS = getCompaniesToNumAcquired(k, BFS_nodeToSortedL2Similarities, acquired_companies_in_graph_by_id, not_acquired_companies_in_graph_by_id)\n",
    "not_acquired_to_num_acquired_neutral = getCompaniesToNumAcquired(k, neutral_nodeToSortedL2Similarities, acquired_companies_in_graph_by_id, not_acquired_companies_in_graph_by_id)\n",
    "not_acquired_to_num_acquired_DFS = getCompaniesToNumAcquired(k, DFS_nodeToSortedL2Similarities, acquired_companies_in_graph_by_id, not_acquired_companies_in_graph_by_id)\n",
    "# Make a plot for each of BFS, neutral, and DFS\n",
    "# x axis will be node id\n",
    "# y axis will be probability under null (binomial) model\n",
    "p = float(len(acquired_companies_in_graph_by_id)) / len(companies_in_graph)\n",
    "BFS_null_model_probabilities = getNullProbs(not_acquired_to_num_acquired_BFS, k, p)\n",
    "neutral_null_model_probabilities = getNullProbs(not_acquired_to_num_acquired_neutral, k, p)\n",
    "DFS_null_model_probabilities = getNullProbs(not_acquired_to_num_acquired_DFS, k, p)\n",
    "\n",
    "\n",
    "print np.mean(not_acquired_to_num_acquired_BFS.values())\n",
    "print np.mean(not_acquired_to_num_acquired_neutral.values())\n",
    "print np.mean(not_acquired_to_num_acquired_DFS.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = range(len(not_acquired_companies_in_graph_by_id))\n",
    "x_vals = sorted(not_acquired_companies_in_graph_by_id)\n",
    "\n",
    "outliers = []\n",
    "y_vals = []\n",
    "for val in x_vals:\n",
    "    y_val = not_acquired_to_num_acquired_DFS[val]\n",
    "    y_vals.append(y_val)\n",
    "    if y_val >= 120: \n",
    "        outliers.append(val)\n",
    "\n",
    "plt.xlabel(\"Node ID\")\n",
    "plt.ylabel(\"Number acquired companies in top \" + str(k))\n",
    "plt.title(\"Nodes vs. number acquired in top \" + str(k) + \" most similar embeddings (DFS)\")\n",
    "plt.scatter(X, y_vals)\n",
    "SUplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns train and test sets with fraction train_frac \n",
    "# of the total number of datapoints in the train set\n",
    "def getData(model, train_frac):\n",
    "    # Use node2vec and logistic regression to make predictions\n",
    "    node_2_vec_embedding_dimension = 128\n",
    "\n",
    "    train_companies = set([])\n",
    "    for company in companies_in_graph: \n",
    "        if np.random.rand() < train_frac: train_companies.add(company)\n",
    "              \n",
    "    test_companies = companies_in_graph.difference(train_companies)\n",
    "\n",
    "    train_X = np.zeros((len(train_companies), node_2_vec_embedding_dimension))\n",
    "    train_Y = np.zeros((len(train_companies),))\n",
    "\n",
    "    train_companies = list(train_companies)\n",
    "    for i in range(len(train_companies)):\n",
    "        company_id = value_to_node_id[train_companies[i]]\n",
    "        company_embedding = model[str(company_id)]\n",
    "        train_X[i] = company_embedding\n",
    "        if company_id in acquired_companies_in_graph_by_id:\n",
    "            train_Y[i] = 1\n",
    "        else:\n",
    "            train_Y[i] = 0\n",
    "\n",
    "    test_X = np.zeros((len(test_companies), node_2_vec_embedding_dimension))\n",
    "    test_Y = np.zeros((len(test_companies),))\n",
    "\n",
    "    test_companies = list(test_companies)\n",
    "    for i in range(len(test_companies)):\n",
    "        company_id = value_to_node_id[test_companies[i]]\n",
    "        company_embedding = model[str(company_id)]\n",
    "        test_X[i] = company_embedding\n",
    "        if company_id in acquired_companies_in_graph_by_id:\n",
    "            test_Y[i] = 1\n",
    "        else:\n",
    "            test_Y[i] = 0 \n",
    "            \n",
    "    return train_X, train_Y, test_X, test_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUPERVISED LEARNING:\n",
    "\n",
    "We'll now use the node2vec embeddings to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_supervised_classifier(train_X, train_Y, test_X, test_Y, fitted_model):\n",
    "    train_predictions = fitted_model.predict(train_X)\n",
    "    test_predictions = fitted_model.predict(test_X)\n",
    "    print \"Train confusion matrix (rows correspond to true labels)\"\n",
    "    print confusion_matrix(train_Y, train_predictions, labels=[0, 1])\n",
    "    print \"Test confusion matrix (rows correspond to true labels)\"\n",
    "    print confusion_matrix(test_Y, test_predictions, labels=[0, 1])\n",
    "\n",
    "    print \"Train score\"\n",
    "    print fitted_model.score(train_X, train_Y)\n",
    "    print \"Test score\"\n",
    "    print fitted_model.score(test_X, test_Y)\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(train_X, train_Y, test_X, test_Y, class_weight):\n",
    "    # Weight the less common class more heavily\n",
    "    # For the confustion matrix, rows correspond to true labels and columns to predicted labels\n",
    "    fitted_model = LogisticRegression(class_weight={0:1, 1:2}).fit(train_X, train_Y)\n",
    "    print \"Successfully fitted logistic regression classifier\"\n",
    "    assess_supervised_classifier(train_X, train_Y, test_X, test_Y, fitted_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like logistic regression mostly predicts not acquired the vast majority of the time as expected\n",
    "# Let's see if a simple neural net can do any better\n",
    "def mlp_classifier(train_X, train_Y, test_X, test_Y):\n",
    "    fitted_model = MLPClassifier().fit(train_X, train_Y)\n",
    "    print \"Successfully fitted MLP classifier\"\n",
    "    assess_supervised_classifier(train_X, train_Y, test_X, test_Y, fitted_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No surprise that the neural net overfit\n",
    "# Let's now try using KNN\n",
    "def knn_classifier(train_X, train_Y, test_X, test_Y, k):\n",
    "    fitted_model = KNeighborsClassifier(n_neighbors=10).fit(train_X, train_Y)\n",
    "    print \"Successfully fitted KNN classifier\"\n",
    "    assess_supervised_classifier(train_X, train_Y, test_X, test_Y, fitted_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll try using a decision tree\n",
    "def decision_tree_classifier(train_X, train_Y, test_X, test_Y):\n",
    "    fitted_model = DecisionTreeClassifier().fit(train_X, train_Y)\n",
    "    print \"Successfully fitted decision tree classifier\"\n",
    "    assess_supervised_classifier(train_X, train_Y, test_X, test_Y, fitted_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we'll give the random forest a go\n",
    "# This model turns a bunch of weaker decision\n",
    "# models into a more powerful ensemble model\n",
    "def random_forest_classifier(train_X, train_Y, test_X, test_Y):\n",
    "    fitted_model = RandomForestClassifier().fit(train_X, train_Y)\n",
    "    print \"Successfully fitted random forest classifier\"\n",
    "    assess_supervised_classifier(train_X, train_Y, test_X, test_Y, fitted_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y, test_X, test_Y = getData(BFS_model, 0.8)\n",
    "\n",
    "logistic_regression(train_X, train_Y, test_X, test_Y, {0:1, 1:2})\n",
    "mlp_classifier(train_X, train_Y, test_X, test_Y)\n",
    "knn_classifier(train_X, train_Y, test_X, test_Y, 10)\n",
    "decision_tree_classifier(train_X, train_Y, test_X, test_Y)\n",
    "random_forest_classifier(train_X, train_Y, test_X, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNSUPERVISED LEARNING:\n",
    "\n",
    "We'll now use the node2vec embeddings to identify clusters within the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_clustering_model(fitted_model, train_X, test_X=None):\n",
    "    train_is_whole_dataset = True if test_X == None else False\n",
    "    \n",
    "    train_cluster_assignments = fitted_model.predict(train_X)\n",
    "    test_cluster_assignments = None if train_is_whole_dataset else fitted_model.predict(test_X)\n",
    "    \n",
    "    # Compute the mean silhoutte score across all samples\n",
    "    # For silhouette scores, the best value is 1 and the worst value is -1\n",
    "    # Values near 0 indicate overlapping clusters\n",
    "    train_silhouette_score = silhouette_score(train_X, train_cluster_assignments)\n",
    "    test_silhouette_score = None if train_is_whole_dataset else silhouette_score(test_X, test_cluster_assignments)\n",
    "    print \"Train silhouette score:\" \n",
    "    print train_silhouette_score\n",
    "    print \"Test silhouette score:\"\n",
    "    print test_silhouette_score\n",
    "\n",
    "    # Compute the mean Calinski-Harabasz index for all samples\n",
    "    # For Calinski-Harabasz, the higher the better\n",
    "    train_ch_score = calinski_harabaz_score(train_X, train_cluster_assignments)\n",
    "    test_ch_score = None if train_is_whole_dataset else calinski_harabaz_score(test_X, test_cluster_assignments)\n",
    "    print \"Train Calinski-Harabasz score:\"\n",
    "    print train_ch_score\n",
    "    print \"Test Calinski-Harabasz score:\"\n",
    "    print test_ch_score\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's assume we don't know anything about the companies\n",
    "# We'll use k-means to cluster their node2vec embeddings\n",
    "def k_means(train_X, test_X, k):\n",
    "    fitted_model = KMeans(n_clusters=k).fit(train_X)\n",
    "    print \"Successfully fitted K Means\"\n",
    "    assess_clustering_model(fitted_model, train_X, test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we try agglomerative clustering\n",
    "# Note that agglomerative clustering has no concept of training\n",
    "def agglomerative_clustering(train_X, test_X, k):\n",
    "    data = np.concatenate((train_X, test_X))\n",
    "    cluster_assignments = AgglomerativeClustering(n_clusters=k).fit_predict(data)\n",
    "    \"Successfully fitted agglomerative clustering\"\n",
    "    assess_clustering_model(fitted_model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DFS appears to be the best for clustering\n",
    "train_X, train_Y, test_X, test_Y = getData(DFS_model, 0.8)\n",
    "\n",
    "k_means(train_X, test_X, 2)\n",
    "agglomerative_clustering(train_X, test_X, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
